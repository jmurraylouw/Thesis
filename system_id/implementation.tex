\section{Implementation and results}

    In order to test the system identification techniques,
    numerous simulations were performed to investigate their performance 
    with different system configurations.
    Firstly, the influence of design parameters on the algorithm performance are explored.
    These parameters include hyperparameters, the length of training data and the algorithm sample time.
    The effect of conditions that are not determined by algorithm design are also explored, 
    like measurement noise, and the physical properties of the payload.
    Finally, the white-box and black-box techniques are tested on a dynamic payload 
    which does not satisfy the assumptions of a simple pendulum.

    \subsection{Methodology}

        \paragraph{Simulation environment}
        A SITL implementation of PX4 \cite{Meier2015} using the Gazebo simulator \cite{Koenig2004} is used to generate data
        for system identification.
        Testing these techniques with simulation data allow us to investigate a much larger range system configurations 
        than possible with practical flights.
        The simulation model used in Gazebo was verified in Chapter~\ref{chap:modelling}.
        Using PX4 in SITL also ensures that the controller dynamics in simulation 
        is as close as possible to practical flights since the same flight stack is used in both cases.
        Gazebo also applies realistic measurement noise to the signals received by PX4, 
        which applies an EKF for state estimation.
        Therefore the the data seen by the system identification techniques 
        will also include the filtering effect of the EKF as it would in practical flights.

        \paragraph{Method overview}
        The procedure used to evaluate the black-box techniques is as follows:
        \begin{enumerate}
            \item Takeoff and hover with the qaudrotor
            \item Start logging input and output data
            \item Command a series of velocity step setpoints with random step sizes and time intervals
            \item Stop logging data
            \item Split data into separate training and testing periods
            \item Build a model from the training data
            \item Calculate a error metric for the model from the testing data
        \end{enumerate}

        \paragraph{PID gains}
        The default PID velocity controller from PX4 is used during these simulation.
        The implemented controller gains are documented in Appendix~\ref{appen:pid_gains}.
        A ROS node is used to read and log the payload angle measurement from Gazebo and a different
        ROS node is used to send the velocity setpoints to PX4 
        through the MAVLink protocol with the ROS package, 'mavros'.

        \paragraph{Steps and intervals}
        A algorithm schedules the series of velocity step commands 
        by assigning random step values and time-intervals within a specified range.
        Theses values are selected from a uniform distribution 
        within the ranges specified in Table~\ref{tbl:input_ranges}
        The maximum velocity step is determined in simulation by iteratively increasing the maximum velocity step 
        to a safe value where the quadrotor remains in stable flight
        and the payload angles do not swing out of control.
        The time interval range is set iteratively to ensure 
        that the generated data includes both transient and steady-state dynamics.
 
        \begin{table}[!h]
            \renewcommand{\arraystretch}{1.1}
            \centering
            \caption{Input data ranges.}
            \begin{tabularx}{0.65\linewidth}{@{}lCCr@{}}
                \toprule
                         & Velocity step [\SI{}{\metre/\second}]  & Step time interval [\SI{}{\second}]\\
                \midrule
                Minimum  & 0                                    & 10\\
                Maximum  & 3                                    & 25\\
                \bottomrule
            \end{tabularx}
            \label{tbl:input_ranges}
        \end{table}
        
        \input{system_id/plots/training_data.tex}

        \paragraph{}
        Figure~\ref{fig:training_data} 
        shows an example of random velocity steps and the resulting velocity response used as training data.
        Using random velocity steps and time intervals prevents the system identification methods 
        from overfitting to a specific set of control conditions.
        The method should rather determine a generalised model 
        that works over a range of possible control conditions.

        \paragraph{}
        The data logged from simulation is then divided into testing and training data.
        The training data is used by the system identification algorithms to generate a regression model 
        and the model is then used to determine a prediction error metric over the unseen testing data.
        It is common practice in model evaluations 
        to use separate sets of data for training and testing.
        This ensures that good predictions scores do not result from models that overfit to the training data. 
        
        \paragraph{}
        The testing data spans a fixed length of time and taken from the start of the simulation period.
        The training data is then allocated from the remainder of the data.
        The same testing data is used to calculate error metrics for different models with different configurations
        to ensure that the metric for each model is comparable. 
        This error metric calculated from the testing data is used to evaluate and rank the performances of different models.       

        % \paragraph{Why velocity steps?}
        % As discussed before, the purpose of identifying a data-driven model of the plant dynamics 
        % is to use it in a MPC controller in the velocity loop, 
        % therefore velocity inputs are used in the system identification process.
        % It is a common technique to use a step responses for system identification \cite{Chen2011} because it exposes a 
        % large range of frequency data in the system dynamics.
        % In other data-driven techniques like Neural-Nets, it may be helpful to use a larger variety of input types.
        % The input schedule may be generated by randomly switching between step, ramp and exponetional inputs 
        % to further gaurd againts overfitting \cite{Kotze2021}.
        % However, for the 
        % In practice it is more common to command position step setpoints since a quadrotor delivers

    \subsection{Error metric}

        \paragraph{}        
        It is common practice is to select a model for a MPC based on k-step-ahead prediction errors \cite{Zhao2014}.
        This is because the model is used to make k-step-ahead predictions during control optimisation.
        When model error is dominated by variance error (caused by disturbances), 
        it may be better to use one-step-prediction error \cite{Zhao2014}.
        However for the quadrotor and payload case it is assumed that variance error 
        (caused by under modelling) dominates the model error.

        \paragraph{}
        Different metrics are used in literature to quantify prediction accuracy for different applications.
        Very common, scale-dependant error metrics are MSE and MAE.
        These metrics are dependant on the unit and scale of a variable, 
        hence they cannot be used to compare predictions of different variables.
        MSE ($l_2$~norm of error values) penalises larger errors more than smaller errors, 
        whereas MAE ($l_1$~norm) penalises errors equally.
        For our use case the $l_1$~norm provides a more intuitive metric than the $l_2$~norm
        because it has the same unit as the prediction variable
        and there is no motivation to penalise larger errors more than smaller errors for our use case.

        \paragraph{}
        MAE is calculated as:
        % \begin{equation}
        %     \bm{MAE} = \frac{1}{N_{run}} \cdot \sum_{k = 1}^{N_{run}} \left\lvert \bm{\hat{x_k}} - \bm{x_k} \right\rvert ,
        % \end{equation}
        \begin{equation}
            \bm{MAE} = mean \left( \phantom{.} \left\lvert \bm{\hat{x_k}} - \bm{x_k} \right\rvert \phantom{.} \right),
        \end{equation}
        where 
        % $N_{run}$ is the number of samples used in the prediction run,
        $\bm{x_k}$ is the actual state vector at time-step $k$, 
        $\bm{\hat{x_k}}$ is the state prediction, 
        and 
        $\bm{MAE}$ is a vector with the MAE of each state.
        
        \paragraph{}
        Popular, scale-free error metrics, like MAPE, MRAE and MASE, are also based on the $l_1$~norm,
        but are independent of the scale and units of a variable \cite{Hyndman2006}.
        These metrics could therefore be used to compare predictions of different variables.
        However, these metrics provide misleading comparisons for our use case.
        MAPE expresses accuracy as the absolute ratio between the error and actual value at each time-step.
        This results in undefined or extremely large values for the payload angle predictions becasue the state has a zero mean.
        The velocity state variable has a non-zero mean, 
        therefore the scale of the MAPE of velocity will significantly different from the MAPE of the payload angle.
        
        \paragraph{}
        MRAE is also popular metric for comparing predictions models used with a MPC \cite{Kaiser2018b},
        however, similarly to MAPE, it also results in undefined values for the payload swing angle.
        MASE does not have this problem and can compare predictions of different variables well, 
        because it expresses accuracy as the ratio between the MAE of the model prediction and the MAE of an in-sample
        naïve forecast \cite{Hyndman2006}.
        However, an in-sample forecast is a naïve prediction for a one-step-ahead prediction, 
        but not for a k-step-ahead prediction.
        Therefore MASE is not a helpful ratio for our use case.

        \paragraph{}
        NMAE\textsubscript{mm} is a scale-free error metric that can compare different variables in our use case.
        It normalises the MAE of a variable by the range of that variable, 
        thereby variables with different means or scales can be compared.
        This value is calculated as:
        % \begin{equation}
        %     metric = \frac{1}{n_x} \cdot \sum_{i = 1}^{n_x} \left( \frac{ \bm{MAE_i} }{ x_{i, max} } \right) 
        % \end{equation}
        \begin{equation}
            NMAE_{mm} = \frac{ MAE }{ x_{max} - x_{min} }
        \end{equation}
        where         
        $x_{i, max}$ and $x_{i, min}$ are the maximum and minimum values 
        of the considered variable in the testing data.

        \paragraph{}
        This results in an error metric for each predicted variable, 
        but a single value is required per model 
        to rank the overall accuracy of different models.
        Therefore the average of the NMAE\textsubscript{mm} of all state variables is used as the single value 
        representing the overall accuracy of a model.
        This is the final error metric used to evaluate the model predictions 
        in the sections to follow.

        % \paragraph{}
        % Other considered error metrics are IAE and ITAE which adds up error over the prediction period.
        % IAE integrates the absolute error over time and weights all error equally.
        % ITAE integrates the absolute error multiplied by time over time 
        % hence errors that occur later in the prediction are weighted more than those that that occur earlier.
        % MAE was chosen over these metrics 
        % since it is closer to the loss function of the MPC than IAE or ITAE.
        % Especially, ITAE achieves the opposite of what is desired because it penalises earlier errors less.
        % For the MPC it is more important that early prediction errors are small since it will reoptimise later  

        \paragraph{}
        Other criteria which are more statistically rigorous in model selection 
        than error metrics are AIC and BIC scores.
        % These are information criteria scores which compute the maximum log likelihood of the model 
        % and add a penalty based on the number of terms in the model.
        Thereby they provide a quantitative way of performing a Pareto analysis, 
        which balances model complexity with model accuracy \cite{Mangan2017}.
        It is generally advantageous to use a parsimonious model, 
        which has a low prediction error but is not overly complex, 
        than a complex model with a slightly lower prediction error.
        This not only helps to avoid overfitting, but also ensures that the MPC optimisation problem 
        is not too computational expensive for the available hardware.
        However, these scores require the computation of the maximum log likelihood of each model 
        over numerous simulations.
        This is computationally intractable and unpractical for our use case 
        because of the large number of hyperparameter combinations to compare, 
        as explained in Section~\ref{sec:hyperparameters}.
        Therefore an error metric will rather be used to evaluate model accuracy.
        
        % because it provides a good measure of a model prediction 
        % and it is computationally fast enough to perform a wide hyperparameter search.
        % AIC or BIC may be able to identify a slightly better model, 
        % but when used with the MPC this slight improvement in prediction accuracy 
        % will result in a negligible improvement in control performance

        \paragraph{}
        The error metric of one model may change significantly 
        different starting conditions or prediction horizons.
        The prediction horizon used for model analysis is selected as \SI{20}{\second}
        which is at least twice as long as the desired MPC prediction horizon.
        Some models have very accurate transient predictions, but prove to be unstable over a longer time horizon.
        If the prediction horizon is too short, these models may score unreasonably low error metrics.
        Selecting these such a model could result in unstable control at certain control conditions.
        Therefore a long prediction horizon is used for testing so that marginally unstable models 
        are penalised heavily in model selection. 

        \murray{Maybe insert example of good initial prediction but bad long term ??}

        \paragraph{}
        Different starting conditions also have a large influence on the prediction score of a model.
        Some models may accurately predict transient behaviour, 
        while being extremely bad at steady-state predictions.
        This would result in an MPC controlling the plant well during the initial step response,
        but becoming unstable during steady-state control.
        In order to have a MPC that can control the plant during the different stages of a flight, 
        a model needs to be selected with accurate predictions over a range of different control conditions.
        
        \paragraph{}
        Therefore the error metric needs to include predictions from multiple starting conditions in the testing data.
        The resulting testing procedure is to first specify a number of equispaced starting conditions within the testing data.
        The model is then run multiple times for the length of the prediction horizon, 
        stating with different initial conditions each time.
        The NMAE\textsubscript{mm} is determined for each run, 
        whereafter the average of these scores gives the final NMAE\textsubscript{mm} score of the model.
        In order to balance the variety of testing conditions with the computational time per error metric calculation, 
        10 was selected as the number of initial conditions used in the final NMAE\textsubscript{mm} score. 
        
        % \paragraph{}
        % Each state error signal is scaled by the reciprocal of the maximum value of that state variable in the training data.
        % % This is to provide a better representative error when taking the mean of state variable errors.
        % This is to ensure that a scale difference in the variable types create a bias in the error metric.
        % For example, the quadrotor velocity reaches values of \SI{3}{\metre/\second} but the payload swing angle has a maximum of only \SI[]{0.526}{\radian}.
        % The velocity prediction error is therefore inherently larger than the payload angle prediction error
        % and will bias the error metric towards favouring models with good velocity predictions.
        % The proposed scaled error metric ensures that the MAE of each state variable can be compared to each other.
        % It also provides an error metric that is better and unbiased representative of the model prediction performance across all state variables. 

    \subsection{Hyperparameters} \label{sec:hyperparameters}
        
        \paragraph{}
        As discussed in Section~\ref{sec:dmdc} and \ref{sec:havok} 
        DMDc and HAVOK models are dependent on two hyperparameters: 
        the number of delay-coordinates, $q$, 
        and the SVD truncation rank, $p$.
        For each system identification run with different system parameters or a different length of training data,
        a hyperparameter search is performed to find the combination of hyperparameters 
        that results the lowest prediction error.
        Firstly, a coarsely spaced grid search is performed with large intervals between tested hyperparameter values.
        The range of tested hyperparameters is then reduced and a finer hyperparameter search is performed.
        From numerous simulation iterations, the range of significant hyperparameter values is conservatively determined to be,
        \begin{equation}
            5 < q < 30, \phantom{--} 5 < p < 50
        \end{equation}.

        \input{system_id/plots/MAE_vs_q.tex} 

        \paragraph{}
        Figure~\ref{fig:NMAE_vs_q} shows the prediction error of DMDc and HAVOK models for different values of $q$.
        For each value of a $q$, a new model is generated with every $p$ in the considered range 
        and the lowest prediction error is plotted.
        There is only a slight difference between the results of DMDc and HAVOK.
        As expected, the models with the least number of terms have the highest prediction errors.
        As the number of terms available to the model increases, the error decreases.
        It is clear that there is a sharp decrease in prediction error for $2 < q < 6$, 
        however there is no longer a significant decrease in error
        as model complexity increases past $q > 12$.        

        \paragraph{}
        This `elbow' in the plot can be considered as the Pareto front, 
        where there is a balance between model complexity and accuracy \cite{Mangan2017}.
        It is desirable to select a parsimonious model on this front that has just enough free parameters 
        to capture the plant dynamics and have good accuracy, 
        without being overly complex \cite{Brunton2019b}.
        These models are less prone to overfitting 
        and also lead to lower computational complexity in the MPC optimisation problem.        

        \input{system_id/plots/singular_values.tex}

        \paragraph{}
        Figure~\ref{fig:singular_values} plots the singular values of the SVD from a HAVOK model in a log scale.
        The singular values of the SVD can be loosely interpreted as 
        a measure of significance of the corresponding POD mode to the plant dynamics \cite{Brunton2019d}. 
        That is, modes with higher singular values contain more relevant information about the plant dynamics 
        than modes with lower singular values.
        The $p$ number of significant singular values, 
        which correspond to the POD modes used to reconstruct the observed dynamics,
        are specifically shown in the plot.
        The truncated singular values are also shown, which correspond to the discarded modes.
        
        \paragraph{}
        The Pareto `elbow' is also visible in this plot where there is noticeable change in gradient
        roughly at the split between significant and truncated values.
        This change in gradient shows that after $p$ modes, 
        there is a significant drop in information contributed per remaining mode.
        Note that the number $p$ was selected from a hyperparameter search using an error metric
        which did not consider the singular values.
        This seems to confirm the notion that the Pareto optimal solution 
        is often the most accurate representation of the actual dynamics \cite{Brunton2019d}.

    \subsection{Sample time}

        \paragraph{}
        The sample time, $T_s$, used for system identification is the sample time of the discrete model, 
        which determines the sample time of the MPC.
        Resampling strategies can enable the MPC to run at a different frequency to the discrete model 
        but this adds unnecessary complexity to the control architecture.
        
        \paragraph{}
        The MPC acts in the velocity loop and commands an acceleration setpoint.
        The default PID velocity controller runs at \SI{50}{\hertz} which corresponds to $T_s =~$\SI{0.02}{\second}.
        Due to the computational complexity of an MPC, the optimiser will struggle to run at \SI{50}{\hertz} on a companion computer on a quadrotor.
        However, the controller needs to run as fast as possible 
        to have significant time-scale separation from the quadrotor dynamics.
        If the controller runs too slowly, it may result in poor flight performance or unstable control.
        The highest natural frequency of the payload based on the range of physical parameters considered 
        is~\SI{8.39}{\hertz} corresponding to a period of \SI{0.119}{\second}.

        \input{system_id/plots/MAE_vs_Ts_vs_L.tex}

        \paragraph{}
        Figure~\ref{fig:MAE_vs_Ts_vs_L} shows the prediction error of different DMD models 
        generated with a range of different sample times.
        The natural frequency of the payload pendulum is dependant on the cable length 
        and influences the frequency response of the plant.
        Therefore Figure~\ref{fig:MAE_vs_Ts_vs_L} plots the experiment result 
        for a different cable lengths to see if it has an effect on the prediction error of the models.

        \paragraph{}
        It appears that for $l =$~\SI{0.25}{\metre}, the prediction error increases for larger sample times.
        This is because the shorter cable length results in a higher natural frequency of the payload,
        hence a faster sample time is required for accurate system identification.
        Note that for all considered cable lengths, the prediction error has a sharp decrease for 
        $T_s >$~\SI{0.045}{\second}.
        This may be because the model does not try to capture the small, high frequency oscilations in the dynamics
        at such slow sample times.
        Hence the long term prediction of the models fits the general shape of the dynamics well and results in low errors.
        However, this sample time is too slow for controlling the practical quadrotor.
        $T_s =$~\SI{0.03}{\second} is selected as the sample time for system identification 
        because it provides a good balance between being fast enough for the quadrotor dynamics and slow enough for a practical MPC implementation.

    \subsection{Choice of payload variable in the state vector}

        \paragraph{}
        As discussed in Section~\ref{sec:plant_considered}, 
        the equations of motion in continuous-time of a floating pendulum are dependent on $\dot{\theta}$ and $V_N$, 
        but are not dependent on $\theta$.
        Therefore it is expected that 
        $
        \bm{x} = \begin{bmatrix}
            V_N & \dot{\theta}
        \end{bmatrix}^T
        $
        will be used as the state vector for system identification.
        However, if $\dot{\theta}$ is not included in the state vector of a discrete model, 
        it can still be represented with numerical differentiation of $\theta$.
        An example of this is the backward Euler form,
        \begin{equation}
            \dot{\theta}_k = (\frac{1}{T_s}) \cdot \theta_k - (\frac{1}{T_s}) \cdot \theta_{k-1} .
        \end{equation}
        Therefore the original state vector can also be replaced by,
        $
            \bm{x} = \begin{bmatrix}
                V_N & \theta
            \end{bmatrix}^T
        $
        for system identification.

        \paragraph{}
        Based on the floating pendulum equations, it is expected that a model derived from $\dot{\theta}$ data 
        will better approximate the actual dynamics than one using $\theta$.
        This is because $\dot{\theta}$ is directly related to the dynamics, 
        compared to $\theta$ which needs to be related to $\dot{\theta}$ to be relevant for the dynamics.
        However, an experiment to compare the performances of these models shows that this has a  negligible effect.

        \input{system_id/plots/different_state_vectors.tex}

        \paragraph{}
        Figure~\ref{fig:different_state_vectors} shows the prediction error of HAVOK models using $\dot{\theta}$ or $\theta$ 
        for a range training data lengths.
        Only for very short lengths of training data, do models using $\dot{\theta}$ outperform those using $\theta$.
        For longer lengths of training data there is a negligible difference in prediction error between the methods.
        Therefore $\theta$ will be used for system identification to avoid unnecessary complexity, 
        since there is no direct measurement of $\dot{\theta}$ on the practical quadrotor.

    \subsection{Noise}

        \paragraph{}
        Measurement noise is bad for system identification because it adds high frequency information to the output signals
        which are not part of the actual dynamics.
        On the practical quadrotor the IMU, barometer, magnetometer and GPS sensors experience measurement noise.
        The EKF performs sensor fusion and smooths out most of the measurement noise 
        to provide a state estimate that is less noisy than raw sensor values.
        Therefore the output from the EKF is used for system identification.
        
        \paragraph{}
        The potentiometer and ADC which measure the payload angle on the quadrotor also has experience measurement noise.
        This signal is not smoothed by an onboard EKF.
        In simulation noise is applied to the payload angle as band-limited white-noise.
        The applied noise power was iteratively adjusted to match that of the practical payload measurements.
        The noisy signals from both the quadrotor EKF and payload swing angle are smoothed 
        with a quadratic regression smoother from MATLAB\textsuperscript{\textregistered} before applying system identification.
        The smoother uses a fixed window length of 20 samples which was selected iteratively 
        to remove high frequency variation without loosing the general shape of the data.

        \input{system_id/plots/acc_sp.tex}
        
        \paragraph{}
        The input signal also needs to be smoothed to remove high frequency noise from the logged signal.
        The quadratic regression smoother does not fit the shape of the input data well 
        because of the sharp, non-differentiable edges in the acceleration setpoint signal.
        Therefore a Gaussian-weighted moving average smoother 
        from MATLAB\textsuperscript{\textregistered} is used for the input signal.
        
        \paragraph{}
        Figure~\ref{fig:acc_sp} shows the North acceleration setpoint for a period of training data.
        Without noise the acceleration setpoint should have a zero mean, 
        however the signal mean shows a constant offset.
        % This offset may be due to the integrators in the controllers correcting for an CoM imbalance to keep the vehicle steady.
        The is due to a measurement offset in the IMU which causes a offset in the orientation vector 
        and therefore affects the control signals.
        The setpoint mean is calculated from the training data and subtracted from the signal to correct for the offset.
        This results in a input signal with a zero mean.
        The calculated mean is reapplied to the MPC control signal during implementation
        to readjust for the required offset.

        % \paragraph{}
        % However, since there is no direct measurement of $\dot{\theta}$, 
        % numerical differentiation is performed on the noisy $\theta$ measurement to estimate $\dot{\theta}$. 
        % This amplifies the noise and results in inaccurate $\dot{\theta}$ signal.
        % Total variation differentiation is implemented to estimate $\dot{\theta}$ from the noisy measurements more accurately.
        % Figure~\ref{fig:payload_noise_diff} shows
        
        % \input{system_id/plots/payload_noise_diff.tex} // With TVDiff 
        
        \input{system_id/plots/noise_vs_no_noise.tex}

        Figure~\ref{fig:noise_vs_no_noise} compares the prediction errors of HAVOK models generated from data with or without noise.
        The plot shows that when using short lengths of training data, 
        the prediction errors are smaller for model generated with noiseless signals.
        However, it appears that the prediction errors are almost equal with longer lengths of training data.
        This is because with a short length of data, the signal variation or energy contributed of the noise is a significant part of the data
        and has a strong influence on the model.
        However, with longer lengths of data, the variation caused by the actual plant dynamics 
        dominates the low energy contribution of the measurement noise. 
        Hence, the noise has a smaller influence on the model.        
        It also appears that at long training data lengths noise has a negligible effect on the prediction error of the resulting models.

        \input{system_id/plots/havok_vs_dmd_noise.tex}

        Figure~\ref{fig:havok_vs_dmd_noise} compares the performance of HAVOK and DMDc model when using noisy data.
        The prediction error curves of the two methods are very simlar, with HAVOK producing slightly lower prediction errors than DMDc.
        However, this difference in error may be so small that it has a negligible effect on control.  

    \subsection{Length of training data}
    
        \paragraph{}
        The length of training data used for system identification affects the quality of the model produced.
        In Figure~\ref{fig:SITL_MAE_vs_train_angular_rate} it is clear that prediction error decreases as the amount of training data increases.
        As more training data is used in the regression problem, 
        the determined model better approximates the actual dynamics because a large range of the dynamics is "seen" by the algorithm.
        
        % \input{low data prediction}
        \paragraph{}
        Models produced from data lengths as short as \SI{5}{\second} predict the movement of state variables surprisingly well.
        % Figure~\ref{} shows prediction.
        Note how the general shape of the prediction represents the training data, 
        even though it contains a lot more high frequency oscillations.

        \input{system_id/plots/MAE_diff.tex}

        \paragraph{}
        % Figure~\ref{} shows the MAE of prediction state derivative.
        Define MAE diff with equation ??.
        % From Figure~\ref{} it appears that at least \SI{}{} training data is required to produce models that represent the dynamics.

        \paragraph{}
        The models produced from HAVOK appear to produce slightly better prediction errors, however this small difference has a negligible effect on control performance.

        \paragraph{}
        % In Figure~\ref{fig:MAE_vs_train} it can be seen that after approximately \SI{??}{\second} 
        the prediction error does not significantly improve with more training data.
        It practice less training data is desirable because less flight time will be wasted on training a model before the quadrotor can fly with a updated controller.
        Less training data also corresponds to lower memory usage on quadrotor hardware.
        Such a slight improvement in prediction error also has a negligible effect on control performance and is therefore not worth the increased data requirement.
        % Therefore, only \SI{??}{\second} of flight data will be used to train system identification models. 


    \subsection{System parameters}
            
        \paragraph{}
        The suspended payload, as described in Section~\ref{sec:plant_considered},
        has two system parameters, $m_p$ and $l$.
        For the practical quadrotor considered, the payload mass is limited to,
        $
            0.1 \leq m_p \leq \SI{0.3}{\kilo\gram} ,
        $
        and the cable length is limited to:
        $
            0.5 \leq l \leq \SI{2}{\metre} .
        $
        Figure~\ref{fig:system_parameters} shows the prediction error of HAVOK models 
        build from simulations with various values of $m_p$ and $l$.
        The plots are not shown for DMDc models because they are so similar to the HAVOK results.

        \input{system_id/plots/system_parameters.tex}

        From Figure~\ref{fig:different_lengths} it seems that there is not a great difference in prediction error 
        for different cable length setups.
        From Figure~\ref{fig:different_masses} it appears that $m_p$ has a greater effect on prediction error,
        since there is a bigger difference in prediction error between plots of different $m_p$ values.
        However, it is clear that the system identification method works for a range of different payload parameters.

        % When no external payload is attached, the connection device attached to the end of the cable is 
        % $m_p = \SI{0.01}{\kilo\gram}$.
        % % It becomes unsafe to Flying without a cable attached to the cable, or with a payload with a very small mass, 
        % % may become unsafe since the cable may not always  kept taut by the mass. 
        % On the other limit, $m_p = \SI{0.4}{\kilo\gram}$ is determined to be the maximum payload mass the quadrotor can carry safely 
        % based on the maximum thrust of the motors.

        % A cable length shorter than \SI{0.5}{\metre} is quite impractical and may rather be attached as a rigid payload.
        % There are very few practical applications that may require a shorter cable length.
        % It is also unsafe to fly with a shorter cable length, 
        % since the payload may collide with the quadrotor during an uncontrolled swing.
        % A longer cable guards against a payload and vehicle collision, 
        % because more energy needs to be transferred to the payload to reach the height of the vehicle. 
        % The maximum cable length is selected as $l=~\SI{2}{\metre}$ by intuition 
        % since a cable much longer than this may not be practically useful for a drone delivery flight with the considered quadrotor.

    \subsection{Dynamic payload} \label{sec:dynamic_payload}
        Some payloads attached to the cable may not satisfy the assumptions made in Section~\ref{sec:plant_considered}.
        For example, if a long payload is attached to the cable, the CoM of the payload will be quite a distance below the attachment of the cable.
        This creates a double pendulum model with dynamics that differ significantly from a single pendulum.
        Figure~?? shows a practical double pendulum use case. 
        In 2D this payload is better represented by the floating double pendulum modelled in Figure~??.

        \murray{insert picture a practical quad with long payload}
        \murray{insert diagram of double pend}
        
        \paragraph{}
        It is a non-trivial task to compare the data-driven models with the white-box models,
        because they are different forms and are used in different types of controllers.
        The a white-box system identification technique determines a continuous state space model 
        which is used in an LQR controller. 
        This is in the form:
        \begin{equation}
            \dot{x} = A x + B u
        \end{equation}
        For this model, it is important that the time-derivative estimate of the model 
        at the current time-step 
        is similar to the actual time-derivative of the system state at that time-step.
        The LQR controller applies an gain to the current state estimate 
        to determine the input signal at that time-step only.
        Hence, the state prediction for multiple time-steps into the future is not as important.

        \paragraph{}
        In contrast, the data-driven techniques result in a discrete state space model,
        \begin{equation}
            x_{k+1} = A x_k + B u_k ,
        \end{equation} 
        which is used in a MPC.
        Therefore the prediction accuracy of the model for multiple time-steps over a time horizon is important,
        since the MPC optimises the control input based on this state prediction.
        
        \paragraph{}
        Even though the models cannot easily be compared to each other, 
        the performance of the controllers using these models can be compared.
        This comparison will be investigated in % Chapter~\ref{chap:}

        \paragraph{}
        For single pend
        plot theta prediction dmd vs havok va white-box

        Note that the payload oscillations are damped slightly by the PID controller.
        It appears that this damping is more complex than the linear damping model 
        used to model the white-box model, since the actual swing amplitude 
        does not linearly decrease as expected.
        The damping effect is a function of the controller gains, 
        the payload connection and the aerodynamic drag of the payload.
        An advantage of the data-driven system identification techniques 
        is that the effect of damping is inherently included in the estimated model 
        without specifically estimating it.
        In contrast, the white-box estimation technique requires a designed algorithm 
        to estimate every parameter that effects the dynamics, 
        namely the payload mass, cable length and damping coefficient


        \paragraph{}
        \input{system_id/plots/prediction_single_pend_black.tex}
        \input{system_id/plots/prediction_single_pend_white.tex}
        
        \input{system_id/plots/prediction_double_pend_black.tex}
        \input{system_id/plots/prediction_double_pend_white.tex}
        
        \input{system_id/plots/FFT_amplitude_single_pend.tex}
        The FFT of the single pendulum leads to an estimated cable length of \SI{1.19}{\metre}        

        \input{system_id/plots/FFT_amplitude_double_pend.tex}
        The FFT of the double pendulum leads to an estimated cable length of \SI{1.39}{\metre}
        
        \paragraph{}
        Since the a priori white-box model is based on a single pendulum model, 
        the dynamics described by the model are significantly different from the actual dynamics.
        This will have a detrimental effect on the control performance of a controller based on such a model,
        since the controller will be designed for different plant than what it is controlling actually controlling.
        
        \input{system_id/plots/MAE_vs_Ntrain_double_pend}

        \paragraph{}
        For each of these payload cases, a different parameter estimation based techniques would needs to be designed for effective control.
        This is undesirable for practical drone deliveries, especially when the type of paylaod is not known well in advance or changes regularly.
        A data-driven technique provides a more general solution since it accommodates a larger range of payload types and does not require a prioir modelling information.
        
        plot hyperparameterss MAE. Not how much more delays are required
        
        Takens theorum.
        Add DMD to plot
        \input{system_id/plots/MAE_vs_q_double_pend}

        \paragraph{}
        The double pendulum is one example of a payload case that would require a redesign of the control architecture.
        Other dynamic payloads that are difficult to model with a white-box method are containers holding a fluid.
        Examples: 

        \paragraph{}
        Another payload case that will cause inaccuracies in estimated white-box model 
        is if a payload is attached rigidly to the quadrotor while also carrying a suspended payload.
        The payload mass estimation is based on the assumption that the quadrotor mass is known.
        However if a mass is rigidly attached to the vehicle, the effective quadrotor mass is changed and the RLS payload mass estimation is no longer accurate.

